<div align="center">
  
## Liver Predictor: Self-supervised Model Predicting Liver Transplantability Based On Transplant Donor Medical Data

University of Zurich, University Hospital of Zurich, ETH AI Center

ICML Jan, NeurIPS May, ECCV Mar 2026

**Yiru Yang, (), Wei Wei, Yong Wang, (prof. Davide Scaramuzza)**


</div>

<br>

## Data Availability

Due to strict clinical governance and patient confidentiality, raw donor data and PDF-derived JSON files cannot be shared. This repository provides:

- complete preprocessing and modeling code
- full data schema from `json`
- synthetic example donors<br>

All experimental results reported in the paper were obtained using confidential hospital-internal data under approved protocols


<p align="left">
  <img src="https://github.com/yiruyang2025/Liver_Predictor/blob/main/assets/1.png" alt="Project 1 Visualization" width="60%">
</p>



## Overview

- Our work follows the DeepMind line of representation-first learning, where structure is learned prior to task supervision, and labels are used only to minimally align representations to downstream decisions under extreme data scarcity
- We learnt A SSL Donor Representation Encoder, and then use the binary transplantability classification (TX vs. NTX) to test

**Key Words**: Representation Learning, Data Scarcity, Self-supervised Learning

<br>

## Structure - Zip file for conference submission

```
Liver_predictor/
â”œâ”€â”€ data/            
â”‚   â”œâ”€â”€ schema.json             
â”‚   â”œâ”€â”€ example_donor.json
â”‚   â””â”€â”€ private/
â”‚       â”œâ”€â”€ donor_001.json
â”‚       â”œâ”€â”€ donor_002.json
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ donor_039.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py           # JSON â†’ feature vector
â”‚   â”œâ”€â”€ ssl_encoder.py           # SSL backbone
â”‚   â”œâ”€â”€ ssl_objectives.py        # contrastive / masking
â”‚   â”œâ”€â”€ classifier.py            # TX / NTX head
â”‚   â”œâ”€â”€ train_ssl.py             # self-supervised pretraining
â”‚   â”œâ”€â”€ train_classifier.py      # downstream prediction
â”‚   â”œâ”€â”€ evaluate.py              # LOOCV / metrics
â”‚   â””â”€â”€ utils.py                 # common helpers
+   â”œâ”€â”€ ablation.py           
+   â””â”€â”€ compare_results.py 
â”‚
â”œâ”€â”€ cross_validation/
â”‚   â”œâ”€â”€ run_loocv.sh             # LOOCV = gold standard when N < 100
+   â””â”€â”€ run_ablation.sh
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

<br>

## Abstract

> LLM stands for Discrete Symbolic Model, but images / depth / point clouds / continuous signals are data in continuous space.<br>

> Forcibly tokenizing continuous space into LLM is incorrect from an information theory perspective. The traditional transformer can only deal with softmax(QKáµ€)V for the information output. <br>

> Fusion Transformer in latent-to-latent for ViT / other cv feature extractor to language tokens alignment. Use Continuous Encoder â†’ Fake Token â†’ Attention Trick to make LLM see the failure of continuous information but still effectively hack it.<br>

> It's like turning an image into 6 random tokens and telling the LLM, â€˜This is image embedding, please pretend you understand itâ€™. This is not multimodal understanding.<br>

<br>

> To address the above fundamental wrongness, we propose â˜… Liver Predictor, a SSL model for liver transplantability prediction under extreme data scarcity.<br>

<br>

## Get Started

### 1. Prerequisites

  - Install CMake (version â‰¥ 3.18)
  - Install dependencies: OpenCV, LibTorch (C++ API for PyTorch)


### 2. Build

```
git clone https://github.com/your-username/Multimodal_Cpp.git  
cd Multimodal_Cpp  
mkdir build && cd build  
cmake ..  
make -j4  
```


## Readings

- [1] Multimodal LLMs for health grounded in individual-specific data, 2023.
- [2] Visual and Semantic Similarity in ImageNet, 2011.
- [3] ğŸ“ ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, 2021.
- [4] SimCLR â€“ Chen et al., ICML 2020.
- [5] BYOL â€“ Grill et al., NeurIPS 2020.
- [6] From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence, 2026.
- [7] DINO â€“ Caron et al., ICCV 2021.
- [8] A Survey on Self-Supervised Representation Learning, 2023.
- [9] TabPFN â€“ Hollmann et al., ICLR 2023.
- [10] A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020.
- [11] Revisiting Deep Learning Models for Tabular Data, NeurIPS 2021.
- [12] Teras: A Unified Deep Learning Library for Tabular Data, https://github.com/KhawajaAbaid/teras


<br><br><br>


